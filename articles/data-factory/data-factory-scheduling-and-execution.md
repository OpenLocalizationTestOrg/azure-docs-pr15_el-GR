<properties
    pageTitle="Προγραμματισμός και εκτέλεσης με εργοστασίου δεδομένων | Microsoft Azure"
    description="Μάθετε τον προγραμματισμό και την εκτέλεση πτυχές των εργοστασίου δεδομένων Azure μοντέλο εφαρμογών."
    services="data-factory"
    documentationCenter=""
    authors="spelluru"
    manager="jhubbard"
    editor="monicar"/>

<tags
    ms.service="data-factory"
    ms.workload="data-services"
    ms.tgt_pltfrm="na"
    ms.devlang="na"
    ms.topic="article"
    ms.date="08/22/2016"
    ms.author="spelluru"/>

# <a name="data-factory-scheduling-and-execution"></a>Προγραμματισμός εργοστασίου δεδομένων και την εκτέλεση
Σε αυτό το άρθρο εξηγεί τις πλευρές τον προγραμματισμό και την εκτέλεση του μοντέλου εφαρμογή εργοστασίου δεδομένων Azure. 

## <a name="prerequisites"></a>Προαπαιτούμενα στοιχεία
Σε αυτό το άρθρο προϋποθέτει ότι έχετε κατανοήσει βασικά στοιχεία του εργοστασίου δεδομένων εφαρμογής μοντέλο έννοιες, συμπεριλαμβανομένων των δραστηριοτήτων, αγωγούς, συνδεδεμένες υπηρεσίες και συνόλων δεδομένων. Για βασικές έννοιες του Azure εργοστασίου δεδομένων, ανατρέξτε στα ακόλουθα άρθρα:

- [Εισαγωγή στην προέλευση δεδομένων](data-factory-introduction.md)
- [Αγωγούς](data-factory-create-pipelines.md)
- [Σύνολα δεδομένων](data-factory-create-datasets.md) 

## <a name="schedule-an-activity"></a>Προγραμματισμός μιας δραστηριότητας

Με την ενότητα χρονοδιάγραμμα της δραστηριότητας JSON, μπορείτε να καθορίσετε ένα επαναλαμβανόμενο χρονοδιάγραμμα για μια δραστηριότητα. Για παράδειγμα, μπορείτε να προγραμματίσετε μια δραστηριότητα κάθε ώρα ως εξής:

    "scheduler": {
        "frequency": "Hour",
        "interval": 1
    },  

![Παράδειγμα χρονοδιαγράμματος](./media/data-factory-scheduling-and-execution/scheduler-example.png)

Όπως φαίνεται στο διάγραμμα, καθορίζοντας ένα χρονοδιάγραμμα για τη δραστηριότητα δημιουργεί μια σειρά από tumbling των windows. Tumbling windows είναι μια σειρά από σταθερού μεγέθους, επικαλύπτονται, συνεχόμενες χρονικά διαστήματα. Αυτά τα windows tumbling λογική για τη δραστηριότητα ονομάζονται *δραστηριότητα των windows*.

Για το τρέχον εκτελεστικό παράθυρο δραστηριότητα, μπορείτε να αποκτήσετε πρόσβαση το χρονικό διάστημα που σχετίζεται με το παράθυρο δραστηριότητας με [WindowStart](data-factory-functions-variables.md#data-factory-system-variables) και [WindowEnd](data-factory-functions-variables.md#data-factory-system-variables) μεταβλητές συστήματος στη δραστηριότητα JSON. Μπορείτε να χρησιμοποιήσετε αυτές τις μεταβλητές για διαφορετικούς λόγους στο JSON τη δραστηριότητά σας. Για παράδειγμα, μπορείτε να χρησιμοποιήσετε τους για να επιλέξετε δεδομένα από εισόδου και εξόδου συνόλων δεδομένων που αναπαριστά την ώρα σειράς δεδομένων.

Η ιδιότητα **scheduler** υποστηρίζει το ίδιο δευτερεύουσες ιδιότητες ως την ιδιότητα **διαθεσιμότητα** σε ένα σύνολο δεδομένων. Για λεπτομέρειες, ανατρέξτε στην ενότητα [διαθεσιμότητα του συνόλου δεδομένων](data-factory-create-datasets.md#Availability) . Παραδείγματα: Προγραμματισμός σε μια συγκεκριμένη χρονική μετατόπιση ή τη ρύθμιση στη λειτουργία για να στοιχίσετε επεξεργασίας στην αρχή ή στο τέλος του χρονικού διαστήματος για το παράθυρο δραστηριότητα.

Μπορείτε να καθορίσετε **το χρονοδιάγραμμα** ιδιοτήτων μιας δραστηριότητας, αλλά αυτή η ιδιότητα είναι **προαιρετικό**. Εάν καθορίσετε μια ιδιότητα, αυτό πρέπει να συμφωνεί με το cadence που καθορίζετε στον ορισμό του συνόλου δεδομένων εξόδου. Προς το παρόν, σύνολο δεδομένων εξόδου είναι τι κατευθύνει το χρονοδιάγραμμα, ώστε να πρέπει να δημιουργήσετε ένα σύνολο δεδομένων εξόδου ακόμα και αν τη δραστηριότητα δεν παράγει κανένα αποτέλεσμα. Εάν η δραστηριότητα μην απαιτηθεί οποιαδήποτε εισαγωγή, μπορείτε να παραλείψετε τη δημιουργία του συνόλου δεδομένων εισόδου.

## <a name="time-series-datasets-and-data-slices"></a>Σύνολα δεδομένων και δεδομένα φέτες σειρά ώρας

Ώρα σειρές δεδομένων είναι συνεχή σειρά σημείων δεδομένων που συνήθως αποτελείται από διαδοχικές μετρήσεις που έγιναν πάνω από ένα χρονικό διάστημα. Συνηθισμένα παραδείγματα ώρα σειρές δεδομένων περιλαμβάνουν αισθητήρα δεδομένα και δεδομένα τηλεμετρίας εφαρμογής.

Με την προέλευση δεδομένων, μπορείτε να επεξεργαστείτε φορά που εκτελείται η σειρά δεδομένων με τον τρόπο μαζικής με τη δραστηριότητα. Συνήθως, είναι μια επαναλαμβανόμενη cadence με τον οποίο φτάνει δεδομένα εισόδου και εξόδου ανάγκες δεδομένων που θα παραχθεί. Σε αυτό το cadence διαμορφώνεται, καθορίζοντας **διαθεσιμότητα** του συνόλου δεδομένων ως εξής:

    "availability": {
      "frequency": "Hour",
      "interval": 1
    },

Κάθε μονάδα δεδομένων που καταναλώθηκε και παράγονται από μια Εκτέλεση δραστηριοτήτων ονομάζεται ένα κομμάτι δεδομένων. Το παρακάτω διάγραμμα παρουσιάζει ένα παράδειγμα μιας δραστηριότητας με ένα σύνολο δεδομένων εισόδου και ένα εξόδου συνόλου δεδομένων. Αυτά τα σύνολα δεδομένων έχει οριστεί σε μια ωριαία συχνότητα **διαθεσιμότητα** .

![Διαθεσιμότητα χρονοδιαγράμματος](./media/data-factory-scheduling-and-execution/availability-scheduler.png)

Το παραπάνω διάγραμμα εμφανίζει τις φέτες ωριαία δεδομένων για το σύνολο δεδομένων εισόδου και εξόδου. Το διάγραμμα εμφανίζει τρεις φέτες εισόδου που είναι έτοιμη για επεξεργασία. Η δραστηριότητα 10-11 Π.Μ βρίσκεται σε εξέλιξη, παράγει στη φέτα εξόδου 10-11 Π.Μ.

Μπορείτε να αποκτήσετε πρόσβαση το χρονικό διάστημα που σχετίζεται με την τρέχουσα φέτα που δημιουργήθηκαν στο του συνόλου δεδομένων JSON με μεταβλητές [SliceStart](data-factory-functions-variables.md#data-factory-system-variables) και [SliceEnd](data-factory-functions-variables.md#data-factory-system-variables).

Προς το παρόν, εργοστασίου δεδομένων απαιτεί ότι το χρονοδιάγραμμα που καθορίζεται στη δραστηριότητα ακριβώς συμφωνεί με το χρονοδιάγραμμα που καθορίζεται στη **διαθεσιμότητα** του συνόλου δεδομένων εξόδου. Γι ' αυτό, **WindowStart**, **WindowEnd**, **SliceStart**και **SliceEnd** πάντα αντιστοίχιση με την ίδια χρονική περίοδο και ένα μεμονωμένο αποτέλεσμα φέτα.

Για περισσότερες πληροφορίες σχετικά με διαφορετικές ιδιότητες που είναι διαθέσιμες για την ενότητα διαθεσιμότητα, ανατρέξτε στο θέμα [Δημιουργία συνόλων δεδομένων](data-factory-create-datasets.md).

## <a name="move-data-from-sql-database-to-blob-storage"></a>Μετακίνηση δεδομένων από βάση δεδομένων SQL με το χώρο αποθήκευσης αντικειμένων Blob

Ας τοποθέτηση ορισμένα πράγματα μεταξύ τους και στην πράξη, δημιουργώντας μια διαδικασία που αντιγράφει δεδομένα από έναν πίνακα βάσης δεδομένων SQL Azure με το χώρο αποθήκευσης αντικειμένων Blob του Azure κάθε ώρα.

**Εισόδου: Σύνολο δεδομένων βάση δεδομένων SQL του Azure**

    {
        "name": "AzureSqlInput",
        "properties": {
            "published": false,
            "type": "AzureSqlTable",
            "linkedServiceName": "AzureSqlLinkedService",
            "typeProperties": {
                "tableName": "MyTable"
            },
            "availability": {
                "frequency": "Hour",
                "interval": 1
            },
            "external": true,
            "policy": {}
        }
    }


**Συχνότητα** έχει οριστεί σε **ώρα** και **διάστημα** έχει οριστεί σε **1** στην ενότητα διαθεσιμότητα.

**Εξόδου: Σύνολο δεδομένων χώρος αποθήκευσης αντικειμένων Blob του Azure**

    {
        "name": "AzureBlobOutput",
        "properties": {
            "published": false,
            "type": "AzureBlob",
            "linkedServiceName": "StorageLinkedService",
            "typeProperties": {
                "folderPath": "mypath/{Year}/{Month}/{Day}/{Hour}",
                "format": {
                    "type": "TextFormat"
                },
                "partitionedBy": [
                    {
                        "name": "Year",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "yyyy"
                        }
                    },
                    {
                        "name": "Month",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "%M"
                        }
                    },
                    {
                        "name": "Day",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "%d"
                        }
                    },
                    {
                        "name": "Hour",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "%H"
                        }
                    }
                ]
            },
            "availability": {
                "frequency": "Hour",
                "interval": 1
            }
        }
    }


**Συχνότητα** έχει οριστεί σε **ώρα** και **διάστημα** έχει οριστεί σε **1** στην ενότητα διαθεσιμότητα.



**Δραστηριότητα: Αντιγράψτε δραστηριότητας**

    {
        "name": "SamplePipeline",
        "properties": {
            "description": "copy activity",
            "activities": [
                {
                    "type": "Copy",
                    "name": "AzureSQLtoBlob",
                    "description": "copy activity",
                    "typeProperties": {
                        "source": {
                            "type": "SqlSource",
                            "sqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "writeBatchSize": 100000,
                            "writeBatchTimeout": "00:05:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "AzureSQLInput"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "AzureBlobOutput"
                        }
                    ],
                    "scheduler": {
                        "frequency": "Hour",
                        "interval": 1
                    }
                }
            ],
            "start": "2015-01-01T08:00:00Z",
            "end": "2015-01-01T11:00:00Z"
        }
    }


Το δείγμα εμφανίζει το Χρονοδιάγραμμα δραστηριότητας και ορίστε συνόλου δεδομένων διαθεσιμότητας ενότητες σε μια ωριαία συχνότητα. Το δείγμα δείχνει πώς μπορείτε να χρησιμοποιήσετε **WindowStart** και **WindowEnd** για να επιλέξετε σχετικά δεδομένα σε δραστηριότητα εκτέλεση και αντιγράψτε την σε ένα αντικείμενο blob με το κατάλληλο **folderPath**. Το **folderPath** είναι με παραμέτρους για να έχετε ένα ξεχωριστό φάκελο για κάθε ώρα.

Κατά την εκτέλεση τρεις από τις φέτες μεταξύ 8-11 π.μ., τα δεδομένα στη βάση δεδομένων SQL Azure είναι ως εξής:

![Δείγμα εισαγωγής](./media/data-factory-scheduling-and-execution/sample-input-data.png)

Μετά την ανάπτυξη της διοχέτευσης, το αντικειμένων blob του Azure συμπληρώνεται ως εξής:

-   Αρχείο 2015/mypath/1/8/1/δεδομένων. &lt;Guid&gt;.txt με δεδομένα

            10002345,334,2,2015-01-01 08:24:00.3130000
            10002345,347,15,2015-01-01 08:24:00.6570000
            10991568,2,7,2015-01-01 08:56:34.5300000

    > [AZURE.NOTE] &lt;GUID&gt; αντικαθίσταται με μια πραγματική guid. Παράδειγμα ονόματος αρχείου: Data.bcde1348-7620-4f93-bb89-0eed3455890b.txt
-   Αρχείο 2015/mypath/1/9/1/δεδομένων. &lt;Guid&gt;.txt με δεδομένα:

            10002345,334,1,2015-01-01 09:13:00.3900000
            24379245,569,23,2015-01-01 09:25:00.3130000
            16777799,21,115,2015-01-01 09:47:34.3130000
-   Αρχείο 2015/mypath/1/10/1/δεδομένων. &lt;Guid&gt;.txt χωρίς δεδομένα.


## <a name="active-period-for-pipeline"></a>Ενεργό περίοδο για διοχέτευσης

[Δημιουργία αγωγούς](data-factory-create-pipelines.md) εισάγονται την έννοια μιας περιόδου ενεργό για μια διαδικασία που καθορίζεται από τη ρύθμιση των ιδιοτήτων **Έναρξη** και **Λήξη** .

Μπορείτε να ορίσετε την ημερομηνία έναρξης για την περίοδο ενεργό διοχέτευσης στο παρελθόν. Εργοστασίου δεδομένων αυτόματα υπολογίζει (πίσω γεμίσματα) όλες τις φέτες δεδομένων στο παρελθόν και ξεκινά την επεξεργασία τους.

## <a name="parallel-processing-of-data-slices"></a>Παράλληλη επεξεργασία φέτες δεδομένων
Μπορείτε να ρυθμίσετε φέτες πίσω διαβαθμισμένο γέμισμα δεδομένων πρέπει να εκτελεστούν σε παράλληλα, ορίζοντας την ιδιότητα **ταυτόχρονης εκτέλεσης** στην ενότητα πολιτική της δραστηριότητας JSON. Για περισσότερες πληροφορίες σχετικά με αυτήν την ιδιότητα, ανατρέξτε στο θέμα [Δημιουργία αγωγούς](data-factory-create-pipelines.md).

## <a name="rerun-a-failed-data-slice"></a>Εκτελέστε ξανά μια φέτα αποτυχίας δεδομένων 
Μπορείτε να παρακολουθείτε εκτέλεσης φέτες με τον τρόπο εμπλουτισμένη, οπτική. Ανατρέξτε στο θέμα [Παρακολούθηση και τη Διαχείριση αγωγούς χρησιμοποιώντας Azure πύλης λεπίδες](data-factory-monitor-manage-pipelines.md) ή [εφαρμογή παρακολούθησης και διαχείρισης](data-factory-monitor-manage-app.md) για λεπτομέρειες.

Εξετάστε το παρακάτω παράδειγμα, η οποία εμφανίζει δύο δραστηριότητες. Activity1 παράγει ένα σύνολο δεδομένων σειράς χρόνου με φέτες ως εξόδου που χρησιμοποιείται ως είσοδο από Activity2 για να προκύψει το τελικό αποτέλεσμα ώρα σειρά dataset.

![Αποτυχία φέτα](./media/data-factory-scheduling-and-execution/failed-slice.png)

Το διάγραμμα δείχνει ότι από τρεις φέτες πρόσφατα, Παρουσιάστηκε σφάλμα παράγουν στη φέτα 9-10 ΠΜ για Dataset2. Δεδομένα εργοστασιακές παρακολουθεί αυτόματα εξάρτηση για του συνόλου δεδομένων σειράς ώρα. Ως αποτέλεσμα, δεν ξεκινά τη δραστηριότητα εκτέλεση για τη μετάδοση φέτα 9-10 ΠΜ.

Εργαλεία παρακολούθησης και διαχείρισης εργοστασίου δεδομένων σάς επιτρέπουν να κάντε Διερεύνηση στα αρχεία καταγραφής διαγνωστικών για την αποτυχία φέτα για να βρείτε τη ρίζα για το πρόβλημα και να το διορθώσω. Αφού έχετε διορθωθεί το πρόβλημα, μπορείτε να ξεκινήσετε εύκολα τη δραστηριότητα εκτέλεση για να προκύψει στη φέτα αποτυχίας. Για περισσότερες λεπτομέρειες σχετικά με τον τρόπο να εκτελέσετε ξανά και να κατανοήσετε μεταβάσεις κατάστασης για φέτες δεδομένων, ανατρέξτε στο θέμα [Παρακολούθηση και τη Διαχείριση αγωγούς χρησιμοποιώντας Azure πύλης λεπίδες](data-factory-monitor-manage-pipelines.md) ή [εφαρμογή παρακολούθησης και διαχείρισης](data-factory-monitor-manage-app.md).

Αφού εκτελέσετε ξανά στη φέτα 9-10 ΠΜ για **Dataset2**, εργοστασίου δεδομένων ξεκινά η εκτέλεση για τη φέτα εξαρτώμενα ΠΜ 9-10 του τελικού συνόλου δεδομένων.

![Επανεκτέλεση αποτυχίας φέτα](./media/data-factory-scheduling-and-execution/rerun-failed-slice.png)

## <a name="run-activities-in-a-sequence"></a>Εκτέλεση δραστηριοτήτων σε μια ακολουθία
Να συνδέετε δύο δραστηριότητες (Εκτέλεση μία δραστηριότητα μετά την άλλη) με τη ρύθμιση του συνόλου δεδομένων εξόδου μία δραστηριότητας ως του συνόλου δεδομένων εισόδου από την άλλη δραστηριότητα. Οι δραστηριότητες που μπορεί να είναι στη διοχέτευση ίδιο ή σε διαφορετικό αγωγούς. Η δεύτερη δραστηριότητα εκτελεί μόνο όταν η το πρώτο ολοκληρωθεί με επιτυχία.

Για παράδειγμα, λάβετε υπόψη τα παρακάτω πεζών-κεφαλαίων:

1.  Διοχέτευση P1 έχει A1 δραστηριότητας που απαιτεί εξωτερικών εισαγωγής dataset D1 και παράγει συνόλου δεδομένων εξόδου D2.
2.  P2 διοχέτευσης έχει A2 δραστηριότητας που απαιτεί την εισαγωγή στοιχείων από το σύνολο δεδομένων D2 και παράγει συνόλου δεδομένων εξόδου D3.

Σε αυτό το σενάριο, δραστηριότητες A1 και A2 βρίσκονται σε διαφορετική αγωγούς. Τη δραστηριότητα A1 εκτελείται, όταν είναι διαθέσιμα τα εξωτερικά δεδομένα και τη συχνότητα προγραμματισμένη διαθεσιμότητα φτάσει. Τη δραστηριότητα A2 εκτελείται κατά τις προγραμματισμένες φέτες από D2 γίνονται διαθέσιμα και είναι καλούν τη συχνότητα προγραμματισμένη διαθεσιμότητα. Εάν υπάρχει ένα σφάλμα σε μία από τις φέτες στο σύνολο δεδομένων D2, A2 δεν εκτελείται για αυτήν τη φέτα μέχρι να είναι διαθέσιμη.

Το διάγραμμα θα μοιάζει η προβολή το παρακάτω διάγραμμα:

![Αλυσιδωτή δραστηριότητες σε δύο αγωγούς](./media/data-factory-scheduling-and-execution/chaining-two-pipelines.png)

Όπως προαναφέρθηκε, οι δραστηριότητες μπορούν να έχουν την ίδια διαδικασία. Προβολή διαγράμματος με δύο δραστηριότητες στο ίδιο διοχέτευση έχει τη μορφή το παρακάτω διάγραμμα:

![Αλυσιδωτή δραστηριότητες στη διοχέτευση ίδια](./media/data-factory-scheduling-and-execution/chaining-one-pipeline.png)

### <a name="copy-sequentially"></a>Αντιγραφή διαδοχικά
Είναι δυνατή η εκτέλεση πολλών λειτουργίες αντιγραφής διαδοχικά με τρόπο διαδοχικές/παραγγελθεί. Για παράδειγμα, μπορεί να έχετε δύο δραστηριότητες αντίγραφο στη διοχέτευση (CopyActivity1 και CopyActivity2) με το εξής σύνολα δεδομένων εξόδου εισαγωγής δεδομένων:   

CopyActivity1

Εισαγωγή δεδομένων: σύνολο δεδομένων. Αποτέλεσμα: Dataset2.

CopyActivity2

Εισαγωγή δεδομένων: Dataset2.  Αποτέλεσμα: Dataset3.

CopyActivity2 θα εκτελεστεί μόνο εάν έχει εκτελεστεί με επιτυχία το CopyActivity1 και Dataset2 είναι διαθέσιμη.

Ακολουθεί η διοχέτευση δείγμα JSON:

    {
        "name": "ChainActivities",
        "properties": {
            "description": "Run activities in sequence",
            "activities": [
                {
                    "type": "Copy",
                    "typeProperties": {
                        "source": {
                            "type": "BlobSource"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "copyBehavior": "PreserveHierarchy",
                            "writeBatchSize": 0,
                            "writeBatchTimeout": "00:00:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "Dataset1"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Dataset2"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00"
                    },
                    "scheduler": {
                        "frequency": "Hour",
                        "interval": 1
                    },
                    "name": "CopyFromBlob1ToBlob2",
                    "description": "Copy data from a blob to another"
                },
                {
                    "type": "Copy",
                    "typeProperties": {
                        "source": {
                            "type": "BlobSource"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "writeBatchSize": 0,
                            "writeBatchTimeout": "00:00:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "Dataset2"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Dataset3"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00"
                    },
                    "scheduler": {
                        "frequency": "Hour",
                        "interval": 1
                    },
                    "name": "CopyFromBlob2ToBlob3",
                    "description": "Copy data from a blob to another"
                }
            ],
            "start": "2016-08-25T01:00:00Z",
            "end": "2016-08-25T01:00:00Z",
            "isPaused": false
        }
    }

Παρατηρήστε ότι στο παράδειγμα, το σύνολο δεδομένων εξόδου της πρώτης δραστηριότητας αντίγραφο (Dataset2) έχει καθοριστεί ως είσοδο για τη δεύτερη δραστηριότητα. Γι ' αυτό, η δεύτερη δραστηριότητα εκτελείται μόνο όταν είναι έτοιμη του συνόλου δεδομένων εξόδου από την πρώτη δραστηριότητα.  

Στο παράδειγμα, CopyActivity2 μπορεί να έχει ένα διαφορετικό εισαγωγής, όπως Dataset3, αλλά μπορείτε να καθορίσετε Dataset2 ως είσοδο σε CopyActivity2, ώστε να τη δραστηριότητα δεν εκτελείται μέχρι να ολοκληρωθεί η CopyActivity1. Για παράδειγμα:

CopyActivity1

Εισαγωγή δεδομένων: Dataset1. Αποτέλεσμα: Dataset2.

CopyActivity2

Εισόδων: Dataset3, Dataset2. Αποτέλεσμα: Dataset4.

    {
        "name": "ChainActivities",
        "properties": {
            "description": "Run activities in sequence",
            "activities": [
                {
                    "type": "Copy",
                    "typeProperties": {
                        "source": {
                            "type": "BlobSource"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "copyBehavior": "PreserveHierarchy",
                            "writeBatchSize": 0,
                            "writeBatchTimeout": "00:00:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "Dataset1"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Dataset2"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00"
                    },
                    "scheduler": {
                        "frequency": "Hour",
                        "interval": 1
                    },
                    "name": "CopyFromBlobToBlob",
                    "description": "Copy data from a blob to another"
                },
                {
                    "type": "Copy",
                    "typeProperties": {
                        "source": {
                            "type": "BlobSource"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "writeBatchSize": 0,
                            "writeBatchTimeout": "00:00:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "Dataset3"
                        },
                        {
                            "name": "Dataset2"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Dataset4"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00"
                    },
                    "scheduler": {
                        "frequency": "Hour",
                        "interval": 1
                    },
                    "name": "CopyFromBlob3ToBlob4",
                    "description": "Copy data from a blob to another"
                }
            ],
            "start": "2017-04-25T01:00:00Z",
            "end": "2017-04-25T01:00:00Z",
            "isPaused": false
        }
    }


Παρατηρήστε ότι στο παράδειγμα, δύο συνόλων δεδομένων εισαγωγής έχουν καθοριστεί για τη δεύτερη δραστηριότητα αντίγραφο. Όταν καθορίζονται πολλές εισόδων, μόνο το πρώτο εισαγωγής dataset χρησιμοποιείται για την αντιγραφή δεδομένων, αλλά άλλα σύνολα δεδομένων που χρησιμοποιούνται ως εξαρτήσεις. CopyActivity2 θα ξεκινήσετε μόνο όταν πληρούνται οι ακόλουθες συνθήκες:

- CopyActivity1 ολοκληρώθηκε με επιτυχία και Dataset2 είναι διαθέσιμη. Σε αυτό το σύνολο δεδομένων δεν χρησιμοποιείται κατά την αντιγραφή δεδομένων σε Dataset4. Λειτουργεί μόνο ως εξάρτησης προγραμματισμού για CopyActivity2.   
- Dataset3 είναι διαθέσιμη. Σε αυτό το σύνολο δεδομένων αντιπροσωπεύει τα δεδομένα που αντιγράφονται στον προορισμό.  



## <a name="model-datasets-with-different-frequencies"></a>Σύνολα δεδομένων μοντέλου με διαφορετικό συχνότητας

Στα δείγματα, της συχνότητας για σύνολα δεδομένων εισόδου και εξόδου και το παράθυρο Προγραμματισμός δραστηριότητας έχουν το ίδιο. Ορισμένα σενάρια απαιτούν τη δυνατότητα να δημιουργήσουν εξόδου σε διαφορετικά από της συχνότητας των εισόδων μία ή περισσότερες συχνότητα. Προέλευση δεδομένων υποστηρίζει μοντελοποίησης αυτά τα σενάρια.

### <a name="sample-1-produce-a-daily-output-report-for-input-data-that-is-available-every-hour"></a>Παράδειγμα 1: Ημερήσια αναφοράς εξόδου για δεδομένα εισόδου που είναι διαθέσιμη κάθε ώρα αγροτικά προϊόντα

Εξετάστε το ενδεχόμενο να ένα σενάριο όπου που έχετε εισαγάγει δεδομένα μέτρησης από αισθητήρες διαθέσιμη κάθε ώρα στο χώρο αποθήκευσης αντικειμένων Blob του Azure. Θέλετε να δημιουργήσετε μια αναφορά ημερήσιων συγκεντρωτικών αποτελεσμάτων με στατιστικά στοιχεία όπως μέσο, μέγιστο και ελάχιστο για την ημέρα με [δεδομένα εργοστασίου hive δραστηριότητας](data-factory-hive-activity.md).

Δείτε τώρα πώς μπορείτε να δημιουργήσετε ένα μοντέλο αυτό το σενάριο με εργοστασίου δεδομένων:

**Εισαγωγή συνόλου δεδομένων**

Τα αρχεία ωριαία εισόδου χάνονται στο φάκελο για την ημέρα που δίνεται. Διαθεσιμότητα για εισαγωγή δεδομένων έχει οριστεί σε **ώρα** (συχνότητα: ώρα, διάστημα: 1).

    {
      "name": "AzureBlobInput",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
          "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
          "partitionedBy": [
            { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
            { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
            { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
          ],
          "format": {
            "type": "TextFormat"
          }
        },
        "external": true,
        "availability": {
          "frequency": "Hour",
          "interval": 1
        }
      }
    }

**Σύνολο δεδομένων εξόδου**

Ένα αρχείο εξόδου δημιουργείται καθημερινά στο φάκελο της ημέρας. Διαθεσιμότητα εξόδου έχει οριστεί στην **ημέρα** (συχνότητα: ημέρα και διάστημα: 1).


    {
      "name": "AzureBlobOutput",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
          "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
          "partitionedBy": [
            { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
            { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
            { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
          ],
          "format": {
            "type": "TextFormat"
          }
        },
        "availability": {
          "frequency": "Day",
          "interval": 1
        }
      }
    }

**Δραστηριότητας: δραστηριότητα σε μια διαδικασία hive**

Η δέσμη ενεργειών ομάδα λαμβάνει τις κατάλληλες πληροφορίες *ημερομηνίας/ώρας* ως παράμετροι που χρησιμοποιούν τη μεταβλητή **WindowStart** , όπως φαίνεται στην το παρακάτω τμήμα κώδικα. Η δέσμη ενεργειών hive χρησιμοποιεί αυτήν τη μεταβλητή για να φορτώσετε τα δεδομένα από τον σωστό φάκελο για την ημέρα και εκτέλεση της συνάθροισης για να δημιουργήσετε το αποτέλεσμα.

        {  
            "name":"SamplePipeline",
            "properties":{  
            "start":"2015-01-01T08:00:00",
            "end":"2015-01-01T11:00:00",
            "description":"hive activity",
            "activities": [
                {
                    "name": "SampleHiveActivity",
                    "inputs": [
                        {
                            "name": "AzureBlobInput"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "AzureBlobOutput"
                        }
                    ],
                    "linkedServiceName": "HDInsightLinkedService",
                    "type": "HDInsightHive",
                    "typeProperties": {
                        "scriptPath": "adftutorial\\hivequery.hql",
                        "scriptLinkedService": "StorageLinkedService",
                        "defines": {
                            "Year": "$$Text.Format('{0:yyyy}',WindowStart)",
                            "Month": "$$Text.Format('{0:%M}',WindowStart)",
                            "Day": "$$Text.Format('{0:%d}',WindowStart)"
                        }
                    },
                    "scheduler": {
                        "frequency": "Day",
                        "interval": 1
                    },          
                    "policy": {
                        "concurrency": 1,
                        "executionPriorityOrder": "OldestFirst",
                        "retry": 2,
                        "timeout": "01:00:00"
                    }
                 }
             ]
           }
        }

Το παρακάτω διάγραμμα παρουσιάζει το σενάριο από μια εξάρτηση δεδομένων άποψη.

![Εξάρτηση δεδομένων](./media/data-factory-scheduling-and-execution/data-dependency.png)

Στη φέτα εξόδου για κάθε ημέρα εξαρτάται από 24 ωριαία φέτες από ένα σύνολο δεδομένων εισόδου. Εργοστασίου δεδομένων υπολογίζει αυτόματα αυτές τις εξαρτήσεις, Κατανόηση των εισαγόμενων δεδομένων φέτες που περιλαμβάνονται στο την ίδια χρονική περίοδο ως στη φέτα εξόδου που θα παραχθεί. Εάν οποιαδήποτε από τις 24 φέτες εισαγωγής δεν είναι διαθέσιμη, εργοστασίου δεδομένων αναμένει στη φέτα εισαγωγής έτοιμο να πριν να ξεκινήσετε τη δραστηριότητα ημερήσια εκτέλεση.


### <a name="sample-2-specify-dependency-with-expressions-and-data-factory-functions"></a>Δείγμα 2: Καθορίστε εξάρτηση με παραστάσεις και τις συναρτήσεις εργοστασίου δεδομένων

Ας δούμε ένα άλλο σενάριο. Ας υποθέσουμε ότι έχετε μια ομάδα δραστηριότητα που επεξεργάζεται δύο συνόλων δεδομένων εισόδου. Ένα από αυτά έχει καθημερινά νέα δεδομένα, αλλά μία από αυτές λαμβάνει νέα δεδομένα κάθε εβδομάδα. Ας υποθέσουμε ότι θέλετε να κάνετε ένα σύνδεσμο κατά μήκος του δύο εισόδων και παράγει αποτέλεσμα καθημερινά.

Η απλή προσέγγιση σε ποια εργοστασίου δεδομένων αυτόματα οι εικόνες από τα δεξιά εισαγωγής φέτες για την επεξεργασία με στοίχιση στο αποτέλεσμα του χρόνου της φέτα δεδομένων περιόδου δεν λειτουργεί.

Πρέπει να καθορίσετε ότι για κάθε δραστηριότητα εκτέλεση, η προέλευση δεδομένων πρέπει να χρησιμοποιήσετε κομμάτι της προηγούμενης εβδομάδας δεδομένων για το σύνολο δεδομένων εβδομαδιαία εισαγωγής. Χρησιμοποιήστε συναρτήσεις εργοστασίου δεδομένων Azure όπως φαίνεται στην το παρακάτω τμήμα κώδικα για την υλοποίηση αυτήν τη συμπεριφορά.

**Input1: Αντικειμένων blob του Azure**

Το πρώτο εισόδου είναι η αντικειμένων blob του Azure ενημερώνονται καθημερινά.

    {
      "name": "AzureBlobInputDaily",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
          "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
          "partitionedBy": [
            { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
            { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
            { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
          ],
          "format": {
            "type": "TextFormat"
          }
        },
        "external": true,
        "availability": {
          "frequency": "Day",
          "interval": 1
        }
      }
    }

**Input2: Αντικειμένων blob του Azure**

Input2 είναι το αντικειμένων blob του Azure ενημερώνονται κάθε εβδομάδα.

    {
      "name": "AzureBlobInputWeekly",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
          "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
          "partitionedBy": [
            { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
            { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
            { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
          ],
          "format": {
            "type": "TextFormat"
          }
        },
        "external": true,
        "availability": {
          "frequency": "Day",
          "interval": 7
        }
      }
    }

**Εξόδου: Αντικειμένων blob του Azure**

Ένα αρχείο εξόδου δημιουργείται καθημερινά στο φάκελο για την ημέρα. Διαθεσιμότητα εξόδου έχει οριστεί σε **ημέρα** (συχνότητα: ημέρα, διάστημα: 1).

    {
      "name": "AzureBlobOutputDaily",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "StorageLinkedService",
        "typeProperties": {
          "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
          "partitionedBy": [
            { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
            { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
            { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
          ],
          "format": {
            "type": "TextFormat"
          }
        },
        "availability": {
          "frequency": "Day",
          "interval": 1
        }
      }
    }

**Δραστηριότητας: δραστηριότητα σε μια διαδικασία hive**

Η δραστηριότητα ομάδα λαμβάνει τα δύο εισόδων και δημιουργεί μια φέτα εξόδου καθημερινά. Μπορείτε να καθορίσετε κάθε ημέρα εξόδου φέτα να εξαρτώνται από την προηγούμενη εβδομάδα εισαγωγής φέτα εβδομαδιαία εισαγωγής ως εξής.

    {  
        "name":"SamplePipeline",
        "properties":{  
        "start":"2015-01-01T08:00:00",
        "end":"2015-01-01T11:00:00",
        "description":"hive activity",
        "activities": [
          {
            "name": "SampleHiveActivity",
            "inputs": [
              {
                "name": "AzureBlobInputDaily"
              },
              {
                "name": "AzureBlobInputWeekly",
                "startTime": "Date.AddDays(SliceStart, - Date.DayOfWeek(SliceStart))",
                "endTime": "Date.AddDays(SliceEnd,  -Date.DayOfWeek(SliceEnd))"  
              }
            ],
            "outputs": [
              {
                "name": "AzureBlobOutputDaily"
              }
            ],
            "linkedServiceName": "HDInsightLinkedService",
            "type": "HDInsightHive",
            "typeProperties": {
              "scriptPath": "adftutorial\\hivequery.hql",
              "scriptLinkedService": "StorageLinkedService",
              "defines": {
                "Year": "$$Text.Format('{0:yyyy}',WindowStart)",
                "Month": "$$Text.Format('{0:%M}',WindowStart)",
                "Day": "$$Text.Format('{0:%d}',WindowStart)"
              }
            },
            "scheduler": {
              "frequency": "Day",
              "interval": 1
            },          
            "policy": {
              "concurrency": 1,
              "executionPriorityOrder": "OldestFirst",
              "retry": 2,  
              "timeout": "01:00:00"
            }
           }
         ]
       }
    }


## <a name="data-factory-functions-and-system-variables"></a>Συναρτήσεις εργοστασίου δεδομένων και μεταβλητές συστήματος   

Ανατρέξτε στο θέμα [συναρτήσεις δεδομένων εργοστασίου και μεταβλητές συστήματος](data-factory-functions-variables.md) για μια λίστα των συναρτήσεων και μεταβλητές συστήματος που υποστηρίζει εργοστασίου δεδομένων.

## <a name="data-dependency-deep-dive"></a>Βαθύ κατάρρευση εξάρτηση δεδομένων

Για να δημιουργήσετε ένα κομμάτι σύνολο δεδομένων από μια Εκτέλεση δραστηριοτήτων, εργοστασίου δεδομένων χρησιμοποιεί το παρακάτω *εξάρτηση μοντέλο* για να καθορίσετε τις σχέσεις μεταξύ των συνόλων δεδομένων που καταναλώθηκε και δημιουργήθηκαν με μια δραστηριότητα.

Το εύρος χρόνου του το εισαγωγής συνόλων δεδομένων που απαιτείται για τη δημιουργία στη φέτα συνόλου δεδομένων εξόδου ονομάζεται *εξάρτηση περίοδο*.

Εκτέλεση μιας δραστηριότητας δημιουργεί ένα σύνολο δεδομένων φέτα μόνο αφού τις φέτες δεδομένων εισαγωγής συνόλων δεδομένων εντός της περιόδου εξάρτηση είναι διαθέσιμες. Με άλλα λόγια, όλες τις φέτες εισαγωγής που περιλαμβάνει την περίοδο εξάρτηση πρέπει να είναι σε κατάσταση **έτοιμη** για τη δραστηριότητα εκτέλεση για να δημιουργήσετε μια φέτα συνόλου δεδομένων εξόδου.

Για να δημιουργήσετε το σύνολο δεδομένων φέτα [**Έναρξη**, **Λήξη**], μια συνάρτηση πρέπει να αντιστοιχίσετε τη φέτα σύνολο δεδομένων για την περίοδο εξάρτηση. Αυτή η συνάρτηση είναι ουσιαστικά ένας τύπος που μετατρέπει την έναρξη και λήξη της στη φέτα σύνολο δεδομένων για την έναρξη και λήξη της περιόδου εξάρτηση. Πιο παλαιότερα:

    DatasetSlice = [start, end]
    DependecyPeriod = [f(start, end), g(start, end)]

**F** και **g** αντιστοίχιση συναρτήσεις που υπολογίζουν την έναρξη και λήξη της περιόδου εξάρτηση για κάθε δραστηριότητα εισόδου.

Όπως φαίνεται στο δείγματα, την περίοδο εξάρτηση είναι ίδια με την περίοδο για τη φέτα δεδομένων που παράγεται. Σε αυτές τις περιπτώσεις, εργοστασίου δεδομένων υπολογίζει αυτόματα τις φέτες εισαγωγής που περιλαμβάνονται στην περίοδο εξάρτηση.  

Για παράδειγμα, στο δείγμα συγκέντρωσης όπου εξόδου παράγεται καθημερινά και δεδομένα εισόδου είναι διαθέσιμη κάθε ώρα, η περίοδος φέτα δεδομένων είναι 24 ώρες. Εργοστασίου δεδομένων εντοπίζει το σχετικό ωριαία εισόδου χωρίζει για συγκεκριμένη χρονική περίοδο και κάνει εξαρτάται από το εισαγωγής φέτα στη φέτα εξόδου.

Μπορείτε επίσης να παρέχετε τις δικές σας αντιστοίχισης για την περίοδο εξάρτηση, όπως φαίνεται στο δείγμα, όπου ένα από τα δεδομένα εισόδου είναι εβδομαδιαία και στη φέτα εξόδου παράγεται καθημερινά.

## <a name="data-dependency-and-validation"></a>Εξάρτηση δεδομένων και επικύρωση

Ένα σύνολο δεδομένων μπορεί να έχει μια πολιτική επικύρωσης που ορίζονται από το, η οποία καθορίζει τον τρόπο μπορεί να επικυρωθεί τα δεδομένα που δημιουργούνται από ένα κομμάτι εκτέλεσης πριν να είναι έτοιμο για κατανάλωση. Για λεπτομέρειες, ανατρέξτε στο θέμα [Δημιουργία συνόλων δεδομένων](data-factory-create-datasets.md) .

Σε αυτές τις περιπτώσεις, αφού ολοκληρωθεί η εκτέλεση, τη φέτα την κατάσταση φέτα εξόδου αλλάζει σε **Αναμονή** με μια δευτερεύουσα κατάσταση **επικύρωσης**. Αφού τις φέτες επικυρώνονται, η κατάσταση φέτα αλλάζει για να **είστε έτοιμοι**.

Εάν ένα κομμάτι δεδομένων έχει παραχθεί, αλλά δεν περάσει την επικύρωση, εκτελείται δραστηριότητας για μετάδοση φέτες που εξαρτώνται από αυτήν τη φέτα δεν επεξεργάζονται.

[Οθόνη και να διαχειριστείτε αγωγούς](data-factory-monitor-manage-pipelines.md) καλύπτει τα διάφορα μέλη φέτες δεδομένων στην προέλευση δεδομένων.

## <a name="external-data"></a>Εξωτερικά δεδομένα

Ένα σύνολο δεδομένων μπορούν να επισημανθούν ως εξωτερικές (όπως φαίνεται στο το παρακάτω τμήμα κώδικα JSON), πραγματική τους καταγωγή· δεν δημιουργήθηκε με την προέλευση δεδομένων. Σε αυτήν την περίπτωση, η πολιτική του συνόλου δεδομένων να έχει ένα πρόσθετο σύνολο των παραμέτρων που περιγράφει επικύρωσης και να επαναλάβετε πολιτικής για το σύνολο δεδομένων. Ανατρέξτε στο θέμα [Δημιουργία αγωγούς](data-factory-create-pipelines.md) για την περιγραφή όλων των ιδιοτήτων.

Παρόμοια με σύνολα δεδομένων που προκύπτουν από την προέλευση δεδομένων, τις φέτες δεδομένων για εξωτερικά δεδομένα πρέπει να είστε έτοιμοι πριν να γίνει επεξεργασία εξαρτώμενα φέτες.

    {
        "name": "AzureSqlInput",
        "properties":
        {
            "type": "AzureSqlTable",
            "linkedServiceName": "AzureSqlLinkedService",
            "typeProperties":
            {
                "tableName": "MyTable"
            },
            "availability":
            {
                "frequency": "Hour",
                "interval": 1     
            },
            "external": true,
            "policy":
            {
                "externalData":
                {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }
            }  
        }
    }


## <a name="onetime-pipeline"></a>Ενημερώσεις διοχέτευσης
Μπορείτε να δημιουργήσετε και να προγραμματίσετε μια διαδικασία για να εκτελέσετε περιοδικά (για παράδειγμα: ανά ώρα ή ημερήσια) κατά τις ώρες έναρξης και λήξης που καθορίζετε στον ορισμό διοχέτευσης. Για λεπτομέρειες, ανατρέξτε στο θέμα [Προγραμματισμός δραστηριότητες](#scheduling-and-execution) . Μπορείτε επίσης να δημιουργήσετε μια διαδικασία που εκτελείται μόνο μία φορά. Για να γίνει αυτό, ορίζετε την ιδιότητα **pipelineMode** στον ορισμό διοχέτευσης για **ενημερώσεις** όπως φαίνεται στο ακόλουθο δείγμα JSON. Η προεπιλεγμένη τιμή για αυτήν την ιδιότητα είναι **προγραμματισμένη**.

    {
        "name": "CopyPipeline",
        "properties": {
            "activities": [
                {
                    "type": "Copy",
                    "typeProperties": {
                        "source": {
                            "type": "BlobSource",
                            "recursive": false
                        },
                        "sink": {
                            "type": "BlobSink",
                            "writeBatchSize": 0,
                            "writeBatchTimeout": "00:00:00"
                        }
                    },
                    "inputs": [
                        {
                            "name": "InputDataset"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "OutputDataset"
                        }
                    ]
                    "name": "CopyActivity-0"
                }
            ]
            "pipelineMode": "OneTime"
        }
    }

Λάβετε υπόψη τα εξής:

- Δεν έχουν καθοριστεί ώρες **έναρξης** και **λήξης** για τη διαδικασία.
- **Διαθεσιμότητα** των συνόλων δεδομένων εισόδου και εξόδου είναι που καθορίζονται (**συχνότητα** και **διάστημα**), ακόμα και αν εργοστασίου δεδομένων δεν χρησιμοποιεί τις τιμές.  
- Προβολή "Διάγραμμα" δεν εμφανίζει αγωγούς εφάπαξ. Αυτή η συμπεριφορά οφείλεται στη σχεδίαση.
- Δεν είναι δυνατό να ενημερωθούν εφάπαξ αγωγούς. Μπορείτε να κλωνοποίηση μια μεμονωμένη διοχέτευσης, μετονομάστε την, ενημέρωση ιδιοτήτων, και ανάπτυξή του για να δημιουργήσετε ένα νέο.
