<properties
    pageTitle="Μετακίνηση δεδομένων από μια εσωτερική SQL Server σε SQL Azure με εργοστασίου δεδομένων Azure | Azure"
    description="Ρυθμίστε ένα διοχέτευσης ADF που συντάσσει δύο δραστηριότητες μετεγκατάστασης δεδομένων που μαζί μετακίνηση δεδομένων σε καθημερινή βάση ανάμεσα σε βάσεις δεδομένων εσωτερικής εγκατάστασης και στο cloud."
    services="machine-learning"
    documentationCenter=""
    authors="bradsev"
    manager="jhubbard"
    editor="cgronlun" />

<tags
    ms.service="machine-learning"
    ms.workload="data-services"
    ms.tgt_pltfrm="na"
    ms.devlang="na"
    ms.topic="article"
    ms.date="09/14/2016"
    ms.author="bradsev" />


# <a name="move-data-from-an-on-premise-sql-server-to-sql-azure-with-azure-data-factory"></a>Μετακίνηση δεδομένων από μια εσωτερική SQL server σε SQL Azure με εργοστασίου δεδομένων Azure

Αυτό το θέμα δείχνει πώς μπορείτε να μετακινήσετε δεδομένα από μια βάση δεδομένων του SQL Server εσωτερικής εγκατάστασης σε μια βάση δεδομένων SQL Azure μέσω χώρο αποθήκευσης Blob του Azure χρησιμοποιώντας το Azure εργοστασίου δεδομένων (ADF).

Το παρακάτω **μενού** συνδέσεις σε θέματα τα οποία περιγράφουν τον τρόπο ingest δεδομένων σε περιβάλλοντα προορισμού όπου τα δεδομένα μπορεί να αποθηκευτεί και να υποβάλλονται σε επεξεργασία κατά τη διαδικασία Science δεδομένων ομάδας.

[AZURE.INCLUDE [cap-ingest-data-selector](../../includes/cap-ingest-data-selector.md)]


## <a name="intro"></a>Εισαγωγή: Τι είναι ADF και πότε πρέπει να να χρησιμοποιηθεί για τη μετεγκατάσταση δεδομένων;

Azure εργοστασίου δεδομένων είναι μια υπηρεσία ενοποίησης πλήρως διαχειριζόμενων δεδομένων που βασίζεται στο cloud που orchestrates και αυτοματοποιεί την κίνηση και Μετασχηματισμός των δεδομένων. Η βασική ιδέα του μοντέλου ADF είναι διοχέτευσης. Μια διαδικασία είναι μια λογική ομαδοποίηση δραστηριότητες, κάθε μία από τις οποίες ορίζει τις ενέργειες που θα αναλάβουν τα δεδομένα που περιέχονται σε σύνολα δεδομένων. Για να ορίσετε τις πληροφορίες που απαιτούνται για την προέλευση δεδομένων για να συνδεθείτε με τους πόρους δεδομένων χρησιμοποιούνται συνδεδεμένες υπηρεσίες.

Με το ADF, μπορεί να αποτελείται υπάρχουσες υπηρεσίες επεξεργασίας δεδομένων σε αγωγούς δεδομένων που είναι ιδιαίτερα διαθέσιμα και διαχειριζόμενων στο cloud. Αυτά τα δεδομένα αγωγούς μπορούν να προγραμματιστούν ingest, προετοιμασία, μετασχηματισμός, ανάλυση και δημοσίευση δεδομένων και ADF διαχειρίζεται και orchestrates τα σύνθετα δεδομένα και εξαρτήσεις επεξεργασίας. Λύσεις μπορεί να δημιουργηθεί γρήγορα και να αναπτυχθεί στο cloud, τη σύνδεση ενός αυξανόμενος αριθμός εσωτερικής εγκατάστασης και cloud προελεύσεις δεδομένων.

Εξετάστε το ενδεχόμενο χρήσης ADF:

- Όταν δεδομένων πρέπει να μετεγκατασταθούν συνεχώς σε ένα σενάριο υβριδική αποκτά πρόσβαση σε πόρους εσωτερικής εγκατάστασης και cloud 
- Όταν τα δεδομένα είναι συναλλαγή ή πρέπει να είναι δυνατή η τροποποίηση ή έχετε επιχειρηματικής λογικής προστεθεί σε αυτό κατά τη μετεγκατάσταση. 

ADF επιτρέπει για τον προγραμματισμό και την παρακολούθηση των εργασιών χρησιμοποιώντας απλές δέσμες ενεργειών JSON που διαχειρίζονται την κυκλοφορία των δεδομένων σε περιοδική βάση. ADF έχει επίσης άλλες δυνατότητες, όπως η υποστήριξη για σύνθετες λειτουργίες. Για περισσότερες πληροφορίες σχετικά με ADF, ανατρέξτε στην τεκμηρίωση στην [Προέλευση δεδομένων Azure (ADF)](https://azure.microsoft.com/services/data-factory/).


## <a name="scenario"></a>Το σενάριο

Μπορούμε να εγκαταστήσουμε μια διοχέτευση ADF που συντάσσει δύο δραστηριότητες μετεγκατάστασης δεδομένων. Μαζί τους μετακίνηση δεδομένων σε καθημερινή βάση ανάμεσα σε μια εσωτερική βάση δεδομένων SQL και μια βάση δεδομένων SQL Azure στο cloud. Οι δύο δραστηριότητες είναι οι εξής:

* Αντιγράψτε δεδομένα από μια βάση δεδομένων SQL Server εσωτερικής εγκατάστασης σε ένα λογαριασμό χώρο αποθήκευσης Blob του Azure
* Αντιγράψτε δεδομένα από το χώρο αποθήκευσης Blob του Azure λογαριασμό σε μια βάση δεδομένων SQL Azure.

>[AZURE.NOTE] Τα βήματα που φαίνεται εδώ έχουν προσαρμοστεί από το πιο λεπτομερείς πρόγραμμα εκμάθησης που παρέχεται από την ομάδα ADF: [Μετακίνηση δεδομένων μεταξύ προελεύσεις εσωτερικής εγκατάστασης και cloud με την πύλη διαχείρισης δεδομένων](../data-factory/data-factory-move-data-between-onprem-and-cloud.md) αναφορές στις σχετικές ενότητες αυτού του θέματος παρέχονται εάν είναι απαραίτητο.


## <a name="prereqs"></a>Προαπαιτούμενα στοιχεία
Αυτό το πρόγραμμα εκμάθησης προϋποθέτει ότι έχετε:

* Μια **συνδρομή Azure**. Εάν δεν έχετε μια συνδρομή, μπορείτε να εγγραφείτε για μια [δωρεάν δοκιμαστική έκδοση](https://azure.microsoft.com/pricing/free-trial/).
* Ένας **λογαριασμός Azure χώρου αποθήκευσης**. Μπορείτε να χρησιμοποιήσετε ένα λογαριασμό Azure χώρου αποθήκευσης για την αποθήκευση των δεδομένων σε αυτό το πρόγραμμα εκμάθησης. Εάν δεν έχετε ένα λογαριασμό Azure χώρου αποθήκευσης, ανατρέξτε στο άρθρο [Δημιουργία λογαριασμού χώρου αποθήκευσης](storage-create-storage-account.md#create-a-storage-account) . Αφού δημιουργήσετε το λογαριασμό χώρου αποθήκευσης, πρέπει να αποκτήσετε το κλειδί λογαριασμού που χρησιμοποιείται για να αποκτήσετε πρόσβαση του χώρου αποθήκευσης. Ανατρέξτε στο θέμα [Προβολή "," Αντιγραφή "και" πλήκτρα πρόσβασης regenerate χώρου αποθήκευσης](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys).
* Πρόσβαση με μια **βάση δεδομένων SQL Azure**. Εάν πρέπει να ρυθμίσετε μια βάση δεδομένων SQL Azure, το tpoic [Γρήγορα αποτελέσματα με βάση δεδομένων SQL Microsoft Azure](../sql-database/sql-database-get-started.md) παρέχει πληροφορίες σχετικά με την παροχή μια νέα παρουσία μιας βάσης δεδομένων SQL Azure.
* Εγκατεστημένη και ρυθμισμένη **Azure PowerShell** τοπικά. Για οδηγίες, ανατρέξτε στο θέμα [Πώς να εγκαταστήσετε και να ρυθμίσετε τις παραμέτρους του PowerShell Azure](../powershell-install-configure.md).

> [AZURE.NOTE] Αυτή η διαδικασία χρησιμοποιεί την [πύλη του Azure](https://portal.azure.com/).


##<a name="upload-data"></a>Αποστολή των δεδομένων του SQL Server εσωτερικής εγκατάστασης

Μπορούμε να χρησιμοποιήσουμε το [σύνολο δεδομένων ταξί νέα ΥΌΡΚΗ](http://chriswhong.com/open-data/foil_nyc_taxi/) για την επίδειξη της διαδικασίας μετεγκατάστασης. Το σύνολο δεδομένων ταξί νέα ΥΌΡΚΗ είναι διαθέσιμη, όπως σημειώνεται σε που δημοσίευση, σε Azure blob χώρος αποθήκευσης [Δεδομένων ταξί νέα ΥΌΡΚΗ](http://www.andresmh.com/nyctaxitrips/). Τα δεδομένα έχουν δύο αρχεία, το αρχείο trip_data.csv, το οποίο περιέχει λεπτομέρειες ταξιδιού, και το αρχείο trip_far.csv, το οποίο περιέχει λεπτομέρειες στο ναύλο που καταβλήθηκε για κάθε ταξίδι. Ένα δείγμα και μια περιγραφή αυτών των αρχείων παρέχονται στα [Νέα ΥΌΡΚΗ ταξί ταξίδια Dataset περιγραφή](machine-learning-data-science-process-sql-walkthrough.md#dataset).


Μπορείτε να προσαρμόσετε τη διαδικασία που παρέχονται εδώ σε ένα σύνολο από τα δικά σας δεδομένα ή να ακολουθήσετε τα βήματα που περιγράφονται με τη χρήση του συνόλου δεδομένων ταξί νέα ΥΌΡΚΗ. Για να αποστείλετε το σύνολο δεδομένων ταξί νέα ΥΌΡΚΗ στη βάση δεδομένων SQL Server εσωτερικής εγκατάστασης, ακολουθήστε τη διαδικασία που περιγράφεται στην [Μαζική εισαγωγή δεδομένων σε βάση δεδομένων SQL Server](machine-learning-data-science-process-sql-walkthrough.md#dbload). Αυτές οι οδηγίες αφορούν στην εικονική μηχανή μια Azure SQL Server, αλλά η διαδικασία για την αποστολή με τον SQL Server εσωτερικής εγκατάστασης είναι η ίδια.


##<a name="create-adf"></a>Δημιουργία ενός εργοστασίου Azure δεδομένων

Τις οδηγίες για τη δημιουργία ενός νέου εργοστασίου δεδομένων Azure και μια ομάδα πόρων στην [πύλη του Azure](https://portal.azure.com/) παρέχονται [Δημιουργία ενός εργοστασίου δεδομένων Azure](../data-factory/data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory). Όνομα τη νέα παρουσία ADF *adfdsp* και το όνομα του πόρου ομάδα δημιουργήσατε *adfdsprg*.


## <a name="install-and-configure-up-the-data-management-gateway"></a>Εγκατάσταση και ρύθμιση παραμέτρων της πύλης διαχείρισης δεδομένων προς τα επάνω

Για να ενεργοποιήσετε το αγωγούς σε μια εργοστασίου Azure δεδομένων για να εργαστείτε με μια εσωτερική SQL Server, πρέπει να την προσθέσετε ως συνδεδεμένο υπηρεσία την προέλευση δεδομένων. Για να δημιουργήσετε ένα συνδεδεμένο υπηρεσίας για τον SQL Server μια εσωτερική εγκατάσταση, θα πρέπει:

- λήψη και εγκατάσταση του Microsoft πύλη διαχείρισης δεδομένων στον υπολογιστή εσωτερικής εγκατάστασης. 
- ρύθμιση παραμέτρων της υπηρεσίας συνδεδεμένων για την προέλευση δεδομένων εσωτερικής εγκατάστασης για να χρησιμοποιήσετε την πύλη. 

Η πύλη διαχείρισης δεδομένων τοποθετεί σειριακά και deserializes τα δεδομένα προέλευσης και δέκτη στον υπολογιστή όπου φιλοξενούνται.

Για οδηγίες εγκατάστασης και λεπτομέρειες στην πύλη διαχείρισης δεδομένων, ανατρέξτε στο θέμα [Μετακίνηση δεδομένων μεταξύ προελεύσεις εσωτερικής εγκατάστασης και cloud με την πύλη διαχείρισης δεδομένων](../data-factory/data-factory-move-data-between-onprem-and-cloud.md)


## <a name="adflinkedservices"></a>Δημιουργία συνδεδεμένων υπηρεσιών για σύνδεση σε πόρους δεδομένων

Μια υπηρεσία συνδεδεμένων καθορίζει τις πληροφορίες που απαιτούνται για το Azure εργοστασίου δεδομένων για να συνδεθείτε με έναν πόρο δεδομένων. Η διαδικασία βήμα προς βήμα για τη δημιουργία συνδεδεμένων υπηρεσιών παρέχεται στη [δημιουργία συνδεδεμένων υπηρεσιών](../data-factory/data-factory-move-data-between-onprem-and-cloud.md#step-2-create-linked-services).

Έχουμε τρεις πόρων σε αυτό το σενάριο για την οποία απαιτούνται συνδεδεμένες υπηρεσίες.

1. [Συνδεδεμένο service για τον SQL Server εσωτερικής εγκατάστασης](#adf-linked-service-onprem-sql)
2. [Συνδεδεμένο υπηρεσίας για το χώρο αποθήκευσης Blob του Azure](#adf-linked-service-blob-store)
3. [Συνδεδεμένο υπηρεσίας για βάση δεδομένων Azure SQL](#adf-linked-service-azure-sql)


###<a name="adf-linked-service-onprem-sql"></a>Συνδεδεμένο υπηρεσίας για βάση δεδομένων SQL Server εσωτερικής εγκατάστασης

Για να δημιουργήσετε την υπηρεσία συνδεδεμένων για τον SQL Server εσωτερικής εγκατάστασης:

- Κάντε κλικ στο **Χώρο αποθήκευσης δεδομένων** στη σελίδα προορισμού ADF στην πύλη κλασική Azure 
- Επιλέξτε **SQL** και πληκτρολογήστε τα διαπιστευτήρια *όνομα χρήστη* και *τον κωδικό πρόσβασης* για τον SQL Server εσωτερικής εγκατάστασης. Πρέπει να εισαγάγετε το όνομα διακομιστή ως ένα **πλήρως προσδιορισμένο όνομα παρουσίας όνομα_διακομιστή ανάστροφη κάθετο (όνομα_διακομιστή\όνομα_περιόδου_λειτουργίας)**. Όνομα το συνδεδεμένο υπηρεσίας *adfonpremsql*.

###<a name="adf-linked-service-blob-store"></a>Συνδεδεμένο υπηρεσίας για Blob

Για να δημιουργήσετε την υπηρεσία συνδεδεμένων για το χώρο αποθήκευσης Blob του Azure λογαριασμό:

- Κάντε κλικ στο **Χώρο αποθήκευσης δεδομένων** στη σελίδα προορισμού ADF στην πύλη κλασική Azure
- Επιλέξτε **Το λογαριασμό χώρου αποθήκευσης Azure** 
- Πληκτρολογήστε το όνομα χώρο αποθήκευσης Blob του Azure λογαριασμό κλειδί και κοντέινερ. Όνομα το συνδεδεμένο υπηρεσίας *adfds*.

###<a name="adf-linked-service-azure-sql"></a>Συνδεδεμένο υπηρεσίας για βάση δεδομένων Azure SQL

Για να δημιουργήσετε την υπηρεσία συνδεδεμένων για τη βάση δεδομένων SQL Azure:

- Κάντε κλικ στο **Χώρο αποθήκευσης δεδομένων** στη σελίδα προορισμού ADF στην πύλη κλασική Azure
- Επιλέξτε **Azure SQL** και εισαγάγετε τα διαπιστευτήρια *όνομα χρήστη* και *τον κωδικό πρόσβασης* για τη βάση δεδομένων SQL Azure. Το *όνομα χρήστη* πρέπει να έχει καθοριστεί ως *user@servername*.   


##<a name="adf-tables"></a>Ορισμός και δημιουργία πινάκων για να καθορίσετε τον τρόπο πρόσβασης τα σύνολα δεδομένων

Δημιουργία πινάκων που καθορίζουν τη δομή, θέση και διαθεσιμότητα τα σύνολα δεδομένων με τις ακόλουθες διαδικασίες που βασίζονται σε δέσμες ενεργειών. Αρχεία JSON χρησιμοποιούνται για τον ορισμό των πινάκων. Για περισσότερες πληροφορίες σχετικά με τη δομή αυτών των αρχείων, ανατρέξτε στο θέμα [συνόλων δεδομένων](../data-factory/data-factory-create-datasets.md).

> [AZURE.NOTE]  Θα πρέπει να εκτελέσετε το `Add-AzureAccount` cmdlet πριν από την εκτέλεση το cmdlet [New-AzureDataFactoryTable](https://msdn.microsoft.com/library/azure/dn835096.aspx) για να επιβεβαιώσετε ότι είναι επιλεγμένο το προς τα δεξιά Azure συνδρομή για την εκτέλεση της εντολής. Για την τεκμηρίωση του αυτό το cmdlet, ανατρέξτε στο θέμα [Προσθήκη AzureAccount](https://msdn.microsoft.com/library/azure/dn790372.aspx).

Οι ορισμοί JSON βασίζεται στους πίνακες χρησιμοποιήσει τα ονόματα των εξής:

* το **όνομα του πίνακα** στον SQL server εσωτερικής εγκατάστασης είναι *nyctaxi_data*
* το **όνομα του κοντέινερ** στο χώρο αποθήκευσης Blob του Azure λογαριασμό είναι *containername*  

Τρεις ορισμοί πίνακα είναι απαραίτητα για αυτό διοχέτευσης ADF:

1. [Πίνακας εσωτερική εγκατάσταση του SQL](#adf-table-onprem-sql)
2. [Πίνακας BLOB](#adf-table-blob-store)
3. [SQL Azure πίνακα](#adf-table-azure-sql)

> [AZURE.NOTE]  Αυτές τις διαδικασίες Χρησιμοποιήστε Azure PowerShell για να ορίσετε και να δημιουργήσετε τις δραστηριότητες ADF. Αλλά αυτές τις εργασίες μπορεί επίσης να πραγματοποιηθεί με την πύλη Azure. Για λεπτομέρειες, ανατρέξτε στο θέμα [Δημιουργία εισόδου και εξόδου συνόλων δεδομένων](../data-factory/data-factory-move-data-between-onprem-and-cloud.md#step-3-create-input-and-output-datasets).

###<a name="adf-table-onprem-sql"></a>Πίνακας εσωτερική εγκατάσταση του SQL

Τον ορισμό του πίνακα για τον SQL Server εσωτερικής εγκατάστασης έχει καθοριστεί στο αρχείο JSON παρακάτω:

        {
            "name": "OnPremSQLTable",
            "properties":
            {
                "location":
                {
                "type": "OnPremisesSqlServerTableLocation",
                "tableName": "nyctaxi_data",
                "linkedServiceName": "adfonpremsql"
                },
                "availability":
                {
                "frequency": "Day",
                "interval": 1,   
                "waitOnExternal":
                {
                "retryInterval": "00:01:00",
                "retryTimeout": "00:10:00",
                "maximumRetry": 3
                }

                }
            }
        }

Τα ονόματα των στηλών δεν περιλαμβάνονται εδώ. Μπορείτε να επιλέξετε δευτερευουσών σε τα ονόματα των στηλών, συμπεριλαμβάνοντας τις εδώ (για λεπτομέρειες ελέγχου στο θέμα [τεκμηρίωση ADF](../data-factory/data-factory-data-movement-activities.md ) .

Αντιγράψτε τον ορισμό JSON του πίνακα σε ένα αρχείο που ονομάζεται αρχείο *onpremtabledef.json* και αποθηκεύστε το σε μια γνωστή θέση (εδώ θεωρείται ότι είναι *C:\temp\onpremtabledef.json*). Δημιουργία πίνακα στο ADF με το ακόλουθο cmdlet του PowerShell Azure:

    New-AzureDataFactoryTable -ResourceGroupName ADFdsprg -DataFactoryName ADFdsp –File C:\temp\onpremtabledef.json


###<a name="adf-table-blob-store"></a>Πίνακας BLOB
Ορισμός για τον πίνακα για τη θέση blob εξόδου είναι τα εξής (χαρτών αναρροφώμενου δεδομένα από εσωτερική στο αντικειμένων blob του Azure):

        {
            "name": "OutputBlobTable",
            "properties":
            {
                "location":
                {
                "type": "AzureBlobLocation",
                "folderPath": "containername",
                "format":
                {
                "type": "TextFormat",
                "columnDelimiter": "\t"
                },
                "linkedServiceName": "adfds"
                },
                "availability":
                {
                "frequency": "Day",
                "interval": 1
                }
            }
        }

Αντιγράψτε τον ορισμό JSON του πίνακα σε ένα αρχείο που ονομάζεται αρχείο *bloboutputtabledef.json* και αποθηκεύστε το σε μια γνωστή θέση (εδώ θεωρείται ότι είναι *C:\temp\bloboutputtabledef.json*). Δημιουργία πίνακα στο ADF με το ακόλουθο cmdlet του PowerShell Azure:

    New-AzureDataFactoryTable -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\temp\bloboutputtabledef.json  

###<a name="adf-table-azure-sq"></a>SQL Azure πίνακα
Ορισμός για τον πίνακα για το SQL Azure εξόδου είναι τα εξής (αυτό το σχήμα αντιστοιχίζει τα δεδομένα που προέρχονται από το αντικείμενο blob):

    {
        "name": "OutputSQLAzureTable",
        "properties":
        {
            "structure":
            [
                { "name": "column1", type": "String"},
                { "name": "column2", type": "String"}                
            ],
            "location":
            {
                "type": "AzureSqlTableLocation",
                "tableName": "your_db_name",
                "linkedServiceName": "adfdssqlazure_linked_servicename"
            },
            "availability":
            {
                "frequency": "Day",
                "interval": 1            
            }
        }
    }

Αντιγράψτε τον ορισμό JSON του πίνακα σε ένα αρχείο που ονομάζεται αρχείο *AzureSqlTable.json* και αποθηκεύστε το σε μια γνωστή θέση (εδώ θεωρείται ότι είναι *C:\temp\AzureSqlTable.json*). Δημιουργία πίνακα στο ADF με το ακόλουθο cmdlet του PowerShell Azure:

    New-AzureDataFactoryTable -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\temp\AzureSqlTable.json  


##<a name="adf-pipeline"></a>Ορισμός και δημιουργία της διοχέτευσης

Καθορίστε τις δραστηριότητες που ανήκουν σε της διοχέτευσης και δημιουργήστε τη διαδικασία με τις ακόλουθες διαδικασίες που βασίζονται σε δέσμες ενεργειών. Για να ορίσετε τις ιδιότητες διοχέτευσης χρησιμοποιείται ένα αρχείο JSON.

* Η δέσμη ενεργειών προϋποθέτει ότι η **σωλήνωση όνομα** είναι *AMLDSProcessPipeline*.
* Επίσης, σημειώστε ότι μπορούμε να εγκαταστήσουμε την περιοδικότητα της τη διαδικασία για να εκτελεστεί σε καθημερινή βάση και να χρησιμοποιήσετε την προεπιλεγμένη ώρα εκτέλεσης του έργου (12 ΠΜ UTC).

> [AZURE.NOTE]Τις ακόλουθες διαδικασίες Χρησιμοποιήστε Azure PowerShell για να ορίσετε και να δημιουργήσετε τη διαδικασία ADF. Ωστόσο, αυτή η εργασία μπορεί επίσης να πραγματοποιηθεί με την πύλη Azure. Για λεπτομέρειες, ανατρέξτε στο θέμα [Δημιουργία και εκτέλεση ενός διοχέτευσης](../data-factory/data-factory-move-data-between-onprem-and-cloud.md#step-4-create-and-run-a-pipeline).

Χρήση των ορισμών πίνακα που παρέχονται προηγουμένως, τον ορισμό διοχέτευσης για το ADF καθορίζεται ως εξής:

        {
            "name": "AMLDSProcessPipeline",
            "properties":
            {
                "description" : "This pipeline has one Copy activity that copies data from an on-premise SQL to Azure blob",
                 "activities":
                [
                    {
                        "name": "CopyFromSQLtoBlob",
                        "description": "Copy data from on-premise SQL server to blob",     
                        "type": "CopyActivity",
                        "inputs": [ {"name": "OnPremSQLTable"} ],
                        "outputs": [ {"name": "OutputBlobTable"} ],
                        "transformation":
                        {
                            "source":
                            {                               
                                "type": "SqlSource",
                                "sqlReaderQuery": "select * from nyctaxi_data"
                            },
                            "sink":
                            {
                                "type": "BlobSink"
                            }   
                        },
                        "Policy":
                        {
                            "concurrency": 3,
                            "executionPriorityOrder": "NewestFirst",
                            "style": "StartOfInterval",
                            "retry": 0,
                            "timeout": "01:00:00"
                        }       

                     },

                    {
                        "name": "CopyFromBlobtoSQLAzure",
                        "description": "Push data to Sql Azure",        
                        "type": "CopyActivity",
                        "inputs": [ {"name": "OutputBlobTable"} ],
                        "outputs": [ {"name": "OutputSQLAzureTable"} ],
                        "transformation":
                        {
                            "source":
                            {                               
                                "type": "BlobSource"
                            },
                            "sink":
                            {
                                "type": "SqlSink",
                                "WriteBatchTimeout": "00:5:00",             
                            }           
                        },
                        "Policy":
                        {
                            "concurrency": 3,
                            "executionPriorityOrder": "NewestFirst",
                            "style": "StartOfInterval",
                            "retry": 2,
                            "timeout": "02:00:00"
                        }
                     }
                ]
            }
        }

Αντιγράψτε αυτόν τον ορισμό JSON από τη διαδικασία σε ένα αρχείο που ονομάζεται αρχείο *pipelinedef.json* και αποθηκεύστε το σε μια γνωστή θέση (εδώ θεωρείται ότι είναι *C:\temp\pipelinedef.json*). Δημιουργία της διοχέτευσης στο ADF με το ακόλουθο cmdlet του PowerShell Azure:

    New-AzureDataFactoryPipeline  -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\temp\pipelinedef.json

Επιβεβαιώστε ότι μπορείτε να δείτε τη διαδικασία στην το ADF στην πύλη κλασική Azure εμφανίζεται ως εξής (όταν κάνετε κλικ στο διάγραμμα)

![](media/machine-learning-data-science-move-sql-azure-adf/DJP1kji.png)


##<a name="adf-pipeline-start"></a>Ξεκινήστε τη διαδικασία
Η διαδικασία μπορεί να εκτελεστεί τώρα χρησιμοποιώντας την ακόλουθη εντολή:

    Set-AzureDataFactoryPipelineActivePeriod -ResourceGroupName ADFdsprg -DataFactoryName ADFdsp -StartDateTime startdateZ –EndDateTime enddateZ –Name AMLDSProcessPipeline

Τις τιμές παραμέτρων *ΗμερομηνίαΈναρξης* και *enddate* πρέπει να αντικατασταθεί με τις πραγματικές ημερομηνίες μεταξύ των οποίων θέλετε τη διαδικασία για να εκτελέσετε.

Όταν εκτελεί τη διαδικασία, θα πρέπει να μπορείτε να δείτε τα δεδομένα που εμφανίζονται στο κοντέινερ επιλεγμένο για το αντικείμενο blob, ένα αρχείο ανά ημέρα.

Σημειώστε ότι θα σας δεν έχουν δυνατή τη λειτουργικότητα που παρέχεται από ADF διοχέτευση δεδομένα σταδιακά. Για περισσότερες πληροφορίες σχετικά με τον τρόπο για να το κάνετε αυτό, καθώς και άλλες δυνατότητες που παρέχονται από ADF, ανατρέξτε στην [τεκμηρίωση ADF](https://azure.microsoft.com/services/data-factory/).
